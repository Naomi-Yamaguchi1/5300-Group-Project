---
title: "Get Raw Data"
author:
    - name: Yuting Fan
      affiliations:
        - name: Georgetown University
date: "04-07-2024"
format:  
  html:
    code-fold: true
    embed-resources: true
editor_options: 
  chunk_output_type: inline
---

```{r}
library(tidyverse)
library(modeldata)
library(leaps)
library(caret)
library(corrplot)
library(ISLR) 
require(boot)
df <- read_csv("data/clean_data/formodel_clean_kidney_disease.csv")
df <- df[,-1]
(df)
```
# Subset selection
```{r}
# split the data
set.seed(123)
train_index <- sample(1:nrow(df), 0.8 * nrow(df))
train <- df[train_index, ]
test <- df[-train_index, ]
```

```{r}
# INSERT CODE 
regfit.best=regsubsets(class~.,train, nvmax=25) # result are the same with method forward/backward
reg.summary=summary(regfit.best)

# Pick the best model
print(c("SUBSET WITH MINIMUM Cp=",which.min(reg.summary$cp)))
print(c("SUBSET WITH MINIMUM BIC=",which.min(reg.summary$bic)))
print(c("SUBSET WITH MAXIMUM adjr2=",which.max(reg.summary$adjr2)))

# the best model
print(coef(regfit.best,9))
print(coef(regfit.best,6))
print(coef(regfit.best,12))
```
```{r}
# cross validation
best_model_index <- which.min(reg.summary$bic)

# Extract the best subset of predictors
best_subset <- names(coef(regfit.best, id = best_model_index))[2:7]

# Define a function for cross-validation
cross_validate <- function(model, X, y, folds) {
  mse_scores <- numeric(length = folds)
  for (i in 1:folds) {
    # Create indices for training and testing sets
    fold_indices <- createFolds(y, k = folds)
    train_index <- unlist(fold_indices[-i])
    test_index <- unlist(fold_indices[i])
    
    # Convert X back to data frame
    X_train <- as.data.frame(X[train_index, ])
    X_test <- as.data.frame(X[test_index, ])
    
    # Fit the model on training data
    model_fit <- lm(y[train_index] ~ ., data = X_train)
    
    # Make predictions on test data
    y_pred <- predict(model_fit, newdata = X_test)
    
    # Calculate MSE
    mse_scores[i] <- mean((y[test_index] - y_pred)^2)
  }
  return(mean(mse_scores))
}

# Extract predictor variables and target variable
X <- model.matrix(class ~ ., data = df)[, -1]
y <- df$class

# Perform cross-validation for the best subset model
best_subset_cv_mse <- cross_validate(NULL, X[, best_subset], y, 5)

cat("Best Subset Model:", best_subset, "\n")
cat("CV MSE for Best Subset Model:", best_subset_cv_mse, "\n")
```

```{r}
# cross validation
best_model_index <- which.min(reg.summary$cp)

# Extract the best subset of predictors
best_subset <- names(coef(regfit.best, id = best_model_index))[2:10]

# Define a function for cross-validation
cross_validate <- function(model, X, y, folds) {
  mse_scores <- numeric(length = folds)
  for (i in 1:folds) {
    # Create indices for training and testing sets
    fold_indices <- createFolds(y, k = folds)
    train_index <- unlist(fold_indices[-i])
    test_index <- unlist(fold_indices[i])
    
    # Convert X back to data frame
    X_train <- as.data.frame(X[train_index, ])
    X_test <- as.data.frame(X[test_index, ])
    
    # Fit the model on training data
    model_fit <- lm(y[train_index] ~ ., data = X_train)
    
    # Make predictions on test data
    y_pred <- predict(model_fit, newdata = X_test)
    
    # Calculate MSE
    mse_scores[i] <- mean((y[test_index] - y_pred)^2)
  }
  return(mean(mse_scores))
}

# Extract predictor variables and target variable
X <- model.matrix(class ~ ., data = df)[, -1]
y <- df$class

# Perform cross-validation for the best subset model
best_subset_cv_mse <- cross_validate(NULL, X[, best_subset], y, 5)

cat("Best Subset Model:", best_subset, "\n")
cat("CV MSE for Best Subset Model:", best_subset_cv_mse, "\n")
```

```{r}
# cross validation
best_model_index <- which.max(reg.summary$adjr2)

# Extract the best subset of predictors
best_subset <- names(coef(regfit.best, id = best_model_index))[2:13]

# Define a function for cross-validation
cross_validate <- function(model, X, y, folds) {
  mse_scores <- numeric(length = folds)
  for (i in 1:folds) {
    # Create indices for training and testing sets
    fold_indices <- createFolds(y, k = folds)
    train_index <- unlist(fold_indices[-i])
    test_index <- unlist(fold_indices[i])
    
    # Convert X back to data frame
    X_train <- as.data.frame(X[train_index, ])
    X_test <- as.data.frame(X[test_index, ])
    
    # Fit the model on training data
    model_fit <- lm(y[train_index] ~ ., data = X_train)
    
    # Make predictions on test data
    y_pred <- predict(model_fit, newdata = X_test)
    
    # Calculate MSE
    mse_scores[i] <- mean((y[test_index] - y_pred)^2)
  }
  return(mean(mse_scores))
}

# Extract predictor variables and target variable
X <- model.matrix(class ~ ., data = df)[, -1]
y <- df$class

# Perform cross-validation for the best subset model
best_subset_cv_mse <- cross_validate(NULL, X[, best_subset], y, 5)

cat("Best Subset Model:", best_subset, "\n")
cat("CV MSE for Best Subset Model:", best_subset_cv_mse, "\n")
```

**Best Subset Model: sg al bu hemo htn dm** 

# Classification
## Logistic regression method
```{r}

```
